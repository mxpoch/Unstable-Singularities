{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40ad296",
   "metadata": {},
   "source": [
    "## Lou-Hou Solutions\n",
    "\n",
    "We are looking for solutions of the profile chosen by Lou. Hou (2014). Starting with a single, purely rotating eddy in a cylinder, they demonstrated evidence of a singularity forming in finite-time. The initial profile required an odd function in the z-direction of the cylinder.\n",
    "\n",
    "They claimed that as the simulation progressed, the singularity will become asymptotically self-similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9a769",
   "metadata": {},
   "source": [
    "## Generalized De Gregorio Equations\n",
    "\n",
    "The De Gregorio equations, specifiaclly the derived CCF equation, has been shown to have a stable singularities in finite time. The generalized De Gregorio equations are:\n",
    "\n",
    "$$\\omega_t + a u \\omega_x = \\omega u_x$$\n",
    "\n",
    "Where $\\omega$ is the vorticity and $u$ is the velocity field. We will be analyzing a particular form of the De Gregorio equation, with $a=-1$. This is also known as the CCF equation. \n",
    "\n",
    "Using the self-similar ansatz for singularities from Wang et. al. (2023):\n",
    "\n",
    "$$\\omega(x,t) = \\frac{1}{1-t}\\Omega(\\frac{x}{(1-t)^{1+\\lambda}})$$\n",
    "\n",
    "The equation is parametrized by $\\lambda$. Then if we define the velocity $u = \\int_0^y H\\Omega ds$, and the change of coordinates $y=\\frac{x}{(1-t)^{1+\\lambda}}$, the De Gregorio equation becomes:\n",
    "\n",
    "$$\\Omega + ((1+\\lambda)y-u)\\frac{\\partial \\Omega}{\\partial y}-\\Omega\\frac{\\partial u}{\\partial y}=0$$\n",
    "\n",
    "Where $a=-1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac184a2",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function is composed of a *condition loss* and a *equation loss* to track the residues of the Boundary Conditions and governing equations respectively. We implicitly impose the odd condition on the function, and the decay to infinity as:\n",
    "\n",
    "$$q = (\\frac{NN_q(z) - NN_q(-z)}{2})\\cdot (1+z^2)^{-1/(2(1+\\lambda))}$$\n",
    "\n",
    "Where $q(z)\\in\\{u(z), \\Omega(z)\\}$.\n",
    "\n",
    "To force the NN away from the trivial solution, we impose normalization conditions:\n",
    "\n",
    "$$g_1 = \\partial_y \\Omega(0)+2$$\n",
    "$$g_2 = \\Omega(0.5) + 0.05$$\n",
    "$$g_3 = \\sum_{y\\in Y_\\infty} \\Omega(y)^2$$\n",
    "\n",
    "Where $g_1, g_2$ normalize the solution away from zero, and $g_3$ guides the function to decay far away from the origin.\n",
    "\n",
    "Due to the nonlocality of the Hilbert Tranform, the De Gregorio equations need to be solved in a large domain. Therefore we define a new $z$-coordinate with the relation:\n",
    "\n",
    "$$y=\\sinh(z) \\iff z=\\sinh^{-1}(y)$$\n",
    "\n",
    "In practice, we sample in the range $d\\in[-30,30]$.\n",
    "\n",
    "The equation losses become:\n",
    "\n",
    "$$f_1 = \\Omega(z) + ((1+\\lambda)\\sinh(z)+au(z)) \\cosh^{-1}(z)\\partial_z[\\Omega(z)] - \\Omega(z) \\cosh^{-1}(z)\\partial_z[u(z)]$$\n",
    "\n",
    "$$f_2 = \\cosh^{-1}(z)\\partial_z[u(z)]-H_n[\\Omega(z)]$$\n",
    "\n",
    "Where $f_2$ was derived from the definition $u_y=H\\Omega$, $H_n$ is the numerical Hilbert transform. \n",
    "\n",
    "For the CCF equations in particular, we can choose to add an additional smoothness condition to distinguish between solutions.\n",
    "\n",
    "$$loss_s = \\frac{1}{N_s}\\sum_{i=1}^{N_s}|\\frac{d^3}{dz^3}(z_i,\\hat q(z_i))|^2$$\n",
    "\n",
    "The final loss function takes the form:\n",
    "\n",
    "$$J(y) = \\hat c_s(\\frac{1}{n_b}\\sum_{j=1}^{n_b}loss^{(j)}_g) + \\hat c_e(\\frac{1}{n_e}\\sum_{k=1}^{n_e}loss^{(k)}_f) + \\hat c_s (\\frac{1}{n_e}\\sum_{k=1}^{n_e}loss^{(k)}_s)$$\n",
    "\n",
    "Where $n_b = 3$, $n_e=2$ are the total number of solution conditions and governing equations used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e43f8c",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "The solutions for $u, \\Omega$ in the CCF equations are solved using a fully-connected neural network with 3 hidden layers and 20 units per hidden layer. $\\tanh(x)$ is used as the activation function. The final layer is an exponential activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804994b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax\n",
    "import numpy as np\n",
    "from hilbert_toolkit import hilbert_haar # not the same high-accuracy hilbert transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96541e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haiku-based NN used to learn profiles for Omega and U\n",
    "# 3 hidden layers with tanh activation functions, and an ELU as the final activation\n",
    "def nnet(z : jax.Array) -> jax.Array: \n",
    "    mlp = hk.Sequential([\n",
    "        hk.Flatten(),\n",
    "        hk.Linear(20), jax.nn.tanh,\n",
    "        hk.Linear(20), jax.nn.tanh,\n",
    "        hk.Linear(20), jax.nn.tanh,\n",
    "        hk.Linear(1), jax.nn.elu\n",
    "    ])\n",
    "    return mlp(z)\n",
    "\n",
    "# defining the output function q for the DG equations\n",
    "def q_DG(z, lambda_val):\n",
    "    if z.ndim == 0:\n",
    "        z = jnp.expand_dims(z, axis=0)\n",
    "\n",
    "    # evaluating the function\n",
    "    nn_z = nnet(z)\n",
    "    nn_neg_z = nnet(-z)\n",
    "    q = (nn_z - nn_neg_z) / 2*(1+z**2)**(-1/(2*(1+lambda_val)))\n",
    "    return jnp.squeeze(q)\n",
    "\n",
    "# making q JAX-compatible\n",
    "q_DG_jax = hk.transform(q_DG)\n",
    "# autodiff gradients\n",
    "dq_dz = jax.grad(q_DG_jax.apply, argnums=2)\n",
    "d3q_dz3 = jax.grad(jax.grad(jax.grad(q_DG_jax.apply, argnums=2), argnums=2), argnums=2)\n",
    "\n",
    "# vmappings\n",
    "in_axes = (None, None, 0, None)\n",
    "q_DG_vmap = jax.vmap(q_DG_jax.apply, in_axes=in_axes)\n",
    "dq_dz_vmap = jax.vmap(dq_dz, in_axes=in_axes)\n",
    "d3q_dz3_vmap = jax.vmap(d3q_dz3, in_axes=in_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c58c02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom high-accuracy Hilbert transform\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "\n",
    "# 2nd order lagrange polynomials\n",
    "def _simpson_weights(n_points):\n",
    "    if n_points % 2 == 0:\n",
    "        raise ValueError(\"Simpson's rule requires an odd number of grid points.\")\n",
    "    \n",
    "    # [1, 4, 2, 4, ..., 2, 4, 1]\n",
    "    weights = jnp.ones(n_points)\n",
    "    weights = weights.at[1::2].set(4.0)\n",
    "    weights = weights.at[2:-1:2].set(2.0)\n",
    "    return weights\n",
    "\n",
    "def jax_simps(y, x):\n",
    "    n_points = y.shape[0]\n",
    "    h = (x[-1] - x[0]) / (n_points - 1)\n",
    "    weights = _simpson_weights(n_points)\n",
    "    return (h / 3.0) * jnp.dot(weights, y)\n",
    "\n",
    "def make_Hn(q_vmap, q, s_grid):    \n",
    "    # Get the domain limits L from the grid\n",
    "    L = s_grid[-1]\n",
    "    \n",
    "    # hilbert transform evaluated at a single z\n",
    "    def _hilbert_fn_internal(params, rng, z, lambda_val):\n",
    "        omega_at_z = q(params, rng, z, lambda_val)\n",
    "        omega_on_grid = q_vmap(params, rng, s_grid, lambda_val)\n",
    "        \n",
    "        # This integrand is now smooth at s=z\n",
    "        integrand = (omega_on_grid - omega_at_z) / (z - s_grid)\n",
    "        integral_part = jax_simps(integrand, s_grid)\n",
    "        \n",
    "        # P.V. integral of 1/(z-s) from -L to L is log(|(L-z)/(L+z)|)\n",
    "        # Small epsilon for numerical stability if z == L or z == -L\n",
    "        epsilon = 1e-10\n",
    "        log_term = jnp.log(jnp.abs((L - z + epsilon) / (L + z + epsilon)))\n",
    "        analytical_part = omega_at_z * log_term\n",
    "        \n",
    "        return (integral_part + analytical_part) / jnp.pi\n",
    "\n",
    "    return _hilbert_fn_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def conditional_loss(Omega_p, rng, lambda_val):\n",
    "      # normalization constant \n",
    "      g1 = (q_DG_jax.apply(Omega_p, rng, 0.5, lambda_val) + 0.05)**2\n",
    "      # uniform sampling of points to decay at infinity\n",
    "      bd_pts = jnp.concatenate([jr.uniform(rng, shape=(10,), minval=29, maxval=30), \n",
    "                              jr.uniform(rng, shape=(10,), minval=-30, maxval=-29)])\n",
    "      g2 = 1/20*jnp.sum(q_DG_vmap(Omega_p, rng, bd_pts, lambda_val)**2)\n",
    "\n",
    "      return 1/2*(g1+g2)\n",
    "\n",
    "# first equation residue\n",
    "def f1(Omega_p, U_p, rng, z, lambda_val):\n",
    "      Omega_z = q_DG_jax.apply(Omega_p, rng, z, lambda_val)\n",
    "      dOmega_dz = dq_dz(Omega_p, rng, z, lambda_val)\n",
    "      U_z = q_DG_jax.apply(U_p, rng, z, lambda_val)\n",
    "      dU_dz = dq_dz(U_p, rng, z, lambda_val)\n",
    "\n",
    "      return (Omega_z + ((1+lambda_val)*jax.nn.sinh(z) - U_z)\n",
    "              *(1/jax.nn.cosh(z))*dOmega_dz - Omega_z*(1/jax.nn.cosh(z))*dU_dz)\n",
    "# taking the 1st and 3rd derivative terms for the smoothness functions \n",
    "df1_dz = jax.grad(f1, argnums=3)\n",
    "d3f1_dz3 = jax.grad(jax.grad(jax.grad(f1, argnums=3), argnums=3), argnums=3)\n",
    "\n",
    "# vmapping\n",
    "f1_vmap = jax.vmap(f1, in_axes=(None, None, None, 0, None))\n",
    "df1_dz_vmap = jax.vmap(df1_dz, in_axes=(None, None, None, 0, None))\n",
    "d3f1_dz3_vmap = jax.vmap(d3f1_dz3, in_axes=(None, None, None, 0, None))\n",
    "\n",
    "# second equation residue\n",
    "# init the custom hilbert transform\n",
    "s_grid = jnp.linspace(-30,30,50001)\n",
    "Hn = make_Hn(q_DG_vmap, q_DG_jax.apply, s_grid)\n",
    "\n",
    "def f2(Omega_p, U_p, rng, z, lambda_val):\n",
    "      dU_dz = dq_dz.apply(U_p, rng, z, lambda_val)\n",
    "      return (1/jax.nn.cosh(z))*dU_dz-Hn(Omega_p, rng, z, lambda_val)\n",
    "# taking the 1st and 3rd derivative terms for the smoothness functions \n",
    "df2_dz = jax.grad(f2, argnums=3)\n",
    "d3f2_dz3 = jax.grad(jax.grad(jax.grad(f2, argnums=3), argnums=3), argnums=3)\n",
    "\n",
    "# vmapping\n",
    "f2_vmap = jax.vmap(f2, in_axes=(None, None, None, 0, None))\n",
    "df2_dz_vmap = jax.vmap(df2_dz, in_axes=(None, None, None, 0, None))\n",
    "d3f2_dz3_vmap = jax.vmap(d3f2_dz3, in_axes=(None, None, None, 0, None))\n",
    "\n",
    "def equation_loss(Omega_p, U_p, rng, lambda_val):\n",
    "      # first equation condition\n",
    "      start_end = jnp.concatenate([jr.uniform(rng, 1, minval=-30, maxval=-20), jr.uniform(rng, 1, minval=20, maxval=30)])\n",
    "      colloc_pts_1 = jnp.linspace(start_end[0], start_end[1], 10000)\n",
    "      f1 = 1/colloc_pts_1.shape[0]*f1_vmap(Omega_p, U_p, rng, colloc_pts_1, lambda_val)**2\n",
    "\n",
    "      # second equation condition\n",
    "      start_end = jnp.concatenate([jr.uniform(rng, 1, minval=-30, maxval=-29), jr.uniform(rng, 1, minval=29, maxval=30)])\n",
    "      colloc_pts_2 = jnp.linspace(start_end[0], start_end[1], 10000)\n",
    "      f2 = 1/colloc_pts_2.shape[0]*f2_vmap(Omega_p, U_p, rng, colloc_pts_2, lambda_val)**2\n",
    "\n",
    "      return 1/2*(f1+f2)\n",
    "\n",
    "def smoothness_loss(Omega_p, U_p, rng, lambda_val):\n",
    "      colloc_pts = jr.uniform(rng, 80, minval=-1, maxval=1)\n",
    "      # df2_dz to find the first smooth lambda value\n",
    "      f1s = 1/colloc_pts.shape[0]*jnp.sum(jnp.abs(df1_dz_vmap(Omega_p, U_p, rng, colloc_pts, lambda_val))**2)\n",
    "      f2s = 1/colloc_pts.shape[0]*jnp.sum(jnp.abs(df2_dz_vmap(Omega_p, U_p, rng, colloc_pts, lambda_val))**2)\n",
    "      return 1/2*(f1s+f2s)\n",
    "\n",
    "# TODO - move colloc pt generation outside of loss functions?\n",
    "def total_loss(trainable_state, rng, lambda_val):\n",
    "      Omega_p = trainable_state['Omega_p']\n",
    "      U_p = trainable_state['U_p']\n",
    "\n",
    "      return (conditional_loss(Omega_p, U_p, rng, lambda_val) \n",
    "              + equation_loss(Omega_p, U_p, rng, lambda_val) \n",
    "              + smoothness_loss(Omega_p, U_p, rng, lambda_val))\n",
    "\n",
    "CCF_val_and_grad = jax.jit(jax.value_and_grad(total_loss, argnums=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions\n",
    "@partial(jax.jit, static_argnames=(\"optimizer\",))\n",
    "def adam_train_step(trainable_state, opt_state, lambda_val, rng, optimizer):\n",
    "    # Get loss and gradients\n",
    "    (loss_val, grads) = CCF_val_and_grad(trainable_state, rng, lambda_val)\n",
    "    \n",
    "    # Update parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    trainable_state = optax.apply_updates(trainable_state, updates)\n",
    "    \n",
    "    return trainable_state, opt_state, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06375f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "import jaxopt\n",
    "# init\n",
    "lambda_val = jnp.array(1.0)\n",
    "key = jax.random.PRKey(11)\n",
    "key, key_init_Omega, key_init_U = jax.random.split(key, 3)\n",
    "Omega_p = q_DG_jax.init(key_init_Omega, jnp.array(0.5), lambda_val)\n",
    "U_p = q_DG_jax.init(key_init_U, jnp.array(0.5), lambda_val)\n",
    "\n",
    "trainable_state = {\n",
    "    \"Omega_p\" : Omega_p,\n",
    "    \"U_p\" : U_p\n",
    "}\n",
    "\n",
    "# First 100k iterations using ADAM\n",
    "LR = 0.001\n",
    "adam = optax.adam(LR)\n",
    "opt_state = adam.init(trainable_state)\n",
    "\n",
    "for i in range():\n",
    "    key, rng_step = jax.random.split(key)\n",
    "    trainable_state, opt_state, loss_val = adam_train_step(trainable_state, opt_state, lambda_val, key, adam)\n",
    "\n",
    "    # updating lambda\n",
    "    dU_dz_0 = dq_dz(trainable_state[\"U_p\"], key, 0, lambda_val)\n",
    "    lambda_val = jnp.array(-2+2*dU_dz_0)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Step {i}, Loss: {loss_val}, Lambda: {lambda_val[0]}\")\n",
    "    \n",
    "\n",
    "# Next 250k interations using L-BFGS\n",
    "rng_lbfgs = jax.random.PRNGKey(0)\n",
    "\n",
    "def lbfgs_loss(state):\n",
    "    # updating the lambda val based on the current state\n",
    "    dU_dz_0 = dq_dz(state[\"U_p\"], key, 0, lambda_val)\n",
    "    lambda_val = jnp.array(-2+2*dU_dz_0)\n",
    "    return total_loss(state, rng_lbfgs, lambda_val)\n",
    "\n",
    "optimizer_lbfgs = jaxopt.LBFGS(\n",
    "    fun=lbfgs_loss, \n",
    "    value_and_grad=True,\n",
    "    maxiter=250000,\n",
    "    tol=1e-8\n",
    ")\n",
    "\n",
    "sol, state_lbfgs = optimizer_lbfgs.run(trainable_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singularity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
