{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40ad296",
   "metadata": {},
   "source": [
    "## Lou-Hou Solutions\n",
    "\n",
    "We are looking for solutions of the profile chosen by Lou. Hou (2014). Starting with a single, purely rotating eddy in a cylinder, they demonstrated evidence of a singularity forming in finite-time. The initial profile required an odd function in the z-direction of the cylinder.\n",
    "\n",
    "They claimed that as the simulation progressed, the singularity will become asymptotically self-similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9a769",
   "metadata": {},
   "source": [
    "## Generalized De Gregorio Equations\n",
    "\n",
    "The De Gregorio equations, specifiaclly the derived CCF equation, has been shown to have a stable singularities in finite time. The generalized De Gregorio equations are:\n",
    "\n",
    "$$\\omega_t + a u \\omega_x = \\omega u_x$$\n",
    "\n",
    "Where $\\omega$ is the vorticity and $u$ is the velocity field. We will be analyzing a particular form of the De Gregorio equation, with $a=-1$. This is also known as the CCF equation. \n",
    "\n",
    "Using the self-similar ansatz for singularities from Wang et. al. (2023):\n",
    "\n",
    "$$\\omega(x,t) = \\frac{1}{1-t}\\Omega(\\frac{x}{(1-t)^{1+\\lambda}})$$\n",
    "\n",
    "The equation is parametrized by $\\lambda$. Then if we define the velocity $u = \\int_0^y H\\Omega ds$, and the change of coordinates $y=\\frac{x}{(1-t)^{1+\\lambda}}$, the De Gregorio equation becomes:\n",
    "\n",
    "$$\\Omega + ((1+\\lambda)y-u)\\frac{\\partial \\Omega}{\\partial y}-\\Omega\\frac{\\partial u}{\\partial y}=0$$\n",
    "\n",
    "Where $a=-1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac184a2",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function is composed of a *condition loss* and a *equation loss* to track the residues of the Boundary Conditions and governing equations respectively. We implicitly impose the odd condition on the function, and the decay to infinity as:\n",
    "\n",
    "$$q = (\\frac{NN_q(z) - NN_q(-z)}{2})\\cdot (1+z^2)^{-1/(2(1+\\lambda))}$$\n",
    "\n",
    "Where $q(z)\\in\\{u(z), \\Omega(z)\\}$.\n",
    "\n",
    "To force the NN away from the trivial solution, we impose normalization conditions:\n",
    "\n",
    "$$g_1 = \\partial_y \\Omega(0)+2$$\n",
    "$$g_2 = \\Omega(0.5) + 0.05$$\n",
    "$$g_3 = \\sum_{y\\in Y_\\infty} \\Omega(y)^2$$\n",
    "\n",
    "Where $g_1, g_2$ normalize the solution away from zero, and $g_3$ guides the function to decay far away from the origin.\n",
    "\n",
    "Due to the nonlocality of the Hilbert Tranform, the De Gregorio equations need to be solved in a large domain. Therefore we define a new $z$-coordinate with the relation:\n",
    "\n",
    "$$y=\\sinh(z) \\iff z=\\sinh^{-1}(y)$$\n",
    "\n",
    "In practice, we sample in the range $d\\in[30,30]$.\n",
    "\n",
    "The equation losses become:\n",
    "\n",
    "$$f_1 = \\Omega(z) + ((1+\\lambda)\\sinh(z)+au(z)) \\cosh^{-1}(z)\\partial_z[\\Omega(z)] - \\Omega(z) \\cosh^{-1}(z)\\partial_z[u(z)]$$\n",
    "\n",
    "$$f_2 = \\cosh^{-1}(z)\\partial_z[u(z)]-H_n[\\Omega(z)]$$\n",
    "\n",
    "Where $f_2$ was derived from the definition $u_y=H\\Omega$, $H_n$ is the numerical Hilbert transform. \n",
    "\n",
    "For the CCF equations in particular, in order to avoid optimizing for local maxima, we add an additional smoothness constraint in the form of a 3rd order loss term:\n",
    "\n",
    "$$loss_s = \\frac{1}{N_s}\\sum_{i=1}^{N_s}|\\frac{d^3}{dy^3}(y_i,\\hat q(y_i))|^2$$\n",
    "\n",
    "The final loss function takes the form:\n",
    "\n",
    "$$J(y) = \\hat c_s(\\frac{1}{n_b}\\sum_{j=1}^{n_b}loss^{(j)}_g) + \\hat c_e(\\frac{1}{n_e}\\sum_{k=1}^{n_e}loss^{(k)}_f) + \\hat c_s (\\frac{1}{n_e}\\sum_{k=1}^{n_e}loss^{(k)}_s)$$\n",
    "\n",
    "Where $n_b = 3$, $n_e=2$ are the total number of solution conditions and governing equations used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e43f8c",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "The solutions for $u, \\Omega$ in the CCF equations are solved using a fully-connected neural network with 6 hidden layers and 30 units per hidden layer. $\\tanh(x)$ is used as the activation function. The final layer is an exponential activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804994b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96541e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haiku-based NN used to learn profiles for Omega and U\n",
    "# 6 hidden layers with tanh activation functions, and an ELU as the final activation\n",
    "def nnet(z : jax.Array) -> jax.Array: \n",
    "    mlp = hk.Sequential([\n",
    "        hk.Flatten(),\n",
    "        hk.Linear(30), jax.nn.tanh,\n",
    "        hk.Linear(30), jax.nn.tanh,\n",
    "        hk.Linear(30), jax.nn.tanh,\n",
    "        hk.Linear(30), jax.nn.tanh,\n",
    "        hk.Linear(30), jax.nn.tanh,\n",
    "        hk.Linear(30), jax.nn.tanh,\n",
    "        hk.Linear(1), jax.nn.elu\n",
    "    ])\n",
    "    return mlp(z)\n",
    "\n",
    "# defining the output function q for the DG equations\n",
    "def q_DG(z, lambda_val):\n",
    "    if z.ndim == 0:\n",
    "        z = jnp.expand_dims(z, axis=0)\n",
    "\n",
    "    # evaluating the function\n",
    "    nn_z = nnet(z)\n",
    "    nn_neg_z = nnet(-z)\n",
    "    q = (nn_z - nn_neg_z) / 2*(1+z**2)**(-1/(2*(1+lambda_val)))\n",
    "    return jnp.squeeze(q)\n",
    "# making q JAX-compatible\n",
    "q_DG_jax = hk.transform(q_DG)\n",
    "\n",
    "# autodiff gradient function wrt z \n",
    "def dq_dz(params,rng):\n",
    "    # fixing params and rng\n",
    "    apply_fn_with_params = lambda z_arg, lambda_arg: q_DG_jax.apply(params, rng, z_arg, lambda_arg)\n",
    "    return jax.grad(apply_fn_with_params, argnums=0)\n",
    "\n",
    "# hilbert transform found on the internet\n",
    "# def Hn(f, grid, x, hilb_grid, h):\n",
    "#     eval_pt = (x - hilb_grid * h) / h\n",
    "#     return jnp.sum(jnp.interp(hilb_grid * h, grid, f) * jnp.sinc(eval_pt / 2) * jnp.sin(eval_pt / 2))\n",
    "\n",
    "# accurate numerical hilbert transform\n",
    "\n",
    "\n",
    "# defining the loss terms\n",
    "def loss(Omega_p, U_p, rng, z, lambda_val):\n",
    "    # conditional losses\n",
    "    # values of z where the NNs should decay to zero\n",
    "    Z_inf = [-30, -29, -28, 28, 29, 30]\n",
    "\n",
    "    # normalization\n",
    "    g1 = q_DG_jax.apply(Omega_p, rng, 0.5, lambda_val) + 0.05\n",
    "    # decay at infinity\n",
    "    g2 = jnp.sum([q_DG_jax.apply(Omega_p, rng, zinf, lambda_val)**2 for zinf in Z_inf])\n",
    "\n",
    "    # equation losses\n",
    "    Omega_z = q_DG_jax.apply(Omega_p, rng, z, lambda_val)\n",
    "    dOmega_dz = dq_dz(Omega_p, rng)\n",
    "    dOmega_dz_z = dOmega_dz_z(z, lambda_val)\n",
    "\n",
    "    U_z = q_DG_jax.apply(U_p, rng, z, lambda_val)\n",
    "    dU_dz = dq_dz(U_p, rng)\n",
    "    dU_dz_z = dU_dz(z, lambda_val)\n",
    "    \n",
    "    f1 = (Omega_z + ((1+lambda_val)*jnp.sinh(z) - U_z)*(1/jnp.cosh(z))*dOmega_dz\n",
    "          - Omega_z*(1/jnp.cosh(z))*dU_dz_z)\n",
    "    f2 = (1/jnp.cosh(z))*dU_dz_z-Hn(Omega_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3478c21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01195263, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "z_in = jnp.array(0.5)\n",
    "lambda_in = jnp.array(1.0)\n",
    "params = q_DG_jax.init(rng, z_in, lambda_in)\n",
    "dqdz = dq_dz(params, rng)\n",
    "dqdz(z_in, lambda_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d88025",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'sech'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msech\u001b[49m(\u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'jax.numpy' has no attribute 'sech'"
     ]
    }
   ],
   "source": [
    "jnp.sech(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3799ee8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-0.03434513, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_DG_jax.apply(params, rng, z_in, lambda_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28471ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JAX + Haiku starter for the Córdoba–Córdoba–Fontelos (CCF) profile search.\n",
    "\n",
    "Features included:\n",
    "  - Haiku MLP implementation (convertible from Flax) with simple mean-zero enforcement.\n",
    "  - Spectral derivative and periodic Hilbert transform implemented via FFT.\n",
    "  - Two-stage precision strategy:\n",
    "      Stage A: pretrain with Adam in float32 (or float64 if jax_enable_x64=True).\n",
    "      Stage B: cast parameters to float64 and run dense Gauss–Newton refinement in float64.\n",
    "    (Comment explains how to go beyond float64 using MPFR/mpmath for critical GN steps.)\n",
    "  - Adam pretraining + full-Jacobian Gauss–Newton refinement using jax.jacrev.\n",
    "\n",
    "Notes:\n",
    "  - JAX support for float128 is limited on most backends. For >64-bit you must use\n",
    "    an external multiprecision library (mpmath / mpfr) to evaluate residuals/Jacobians\n",
    "    outside JAX and then solve the linear system in high precision. I include notes\n",
    "    and a small helper showing how to export the residual function for that purpose.\n",
    "  - Dense GN is expensive; for large networks switch to Jacobian-vector products and CG.\n",
    "\n",
    "Run: python jax_ccf_haiku.py\n",
    "\n",
    "Dependencies:\n",
    "  - jax, jaxlib\n",
    "  - dm-haiku\n",
    "  - optax\n",
    "  - numpy\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from functools import partial\n",
    "\n",
    "# Haiku imports\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "# Optionally enable 64-bit for Stage B (do this before creating arrays if you want x64 everywhere)\n",
    "# jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "# ----------------------\n",
    "# Domain\n",
    "# ----------------------\n",
    "\n",
    "N = 512\n",
    "x = jnp.linspace(-0.5, 0.5, N, endpoint=False)\n",
    "dx = x[1] - x[0]\n",
    "\n",
    "rng = random.PRNGKey(42)\n",
    "\n",
    "# ----------------------\n",
    "# Haiku MLP\n",
    "# ----------------------\n",
    "\n",
    "def mlp_fn(x, hidden=(128,128,128)):\n",
    "    h = x\n",
    "    for size in hidden:\n",
    "        h = hk.Linear(size)(h)\n",
    "        h = jax.nn.relu(h)\n",
    "    h = hk.Linear(1)(h)\n",
    "    return jnp.squeeze(h, -1)\n",
    "\n",
    "net = hk.without_apply_rng(hk.transform(mlp_fn))\n",
    "\n",
    "# wrapper to evaluate network on grid and enforce mean-zero\n",
    "\n",
    "def net_forward(params, x_grid):\n",
    "    xin = x_grid.reshape(-1,1)\n",
    "    y = net.apply(params, xin)\n",
    "    # enforce mean-zero (common for CCF)\n",
    "    y = y - jnp.mean(y)\n",
    "    return y\n",
    "\n",
    "# ----------------------\n",
    "# Profile residual (CCF self-similar, kappa=1)\n",
    "# R = -Phi - x Phi' - (H Phi) Phi'\n",
    "# ----------------------\n",
    "\n",
    "@jit\n",
    "def profile_residual_haiku(flat_params, unravel_fn, x_grid):\n",
    "    params = unravel_fn(flat_params)\n",
    "    Phi = net_forward(params, x_grid)\n",
    "    Phi_x = spectral_derivative(Phi)\n",
    "    HPhi = hilbert_transform(Phi)\n",
    "    R = -Phi - x_grid * Phi_x - (HPhi) * Phi_x\n",
    "    return R\n",
    "\n",
    "# ----------------------\n",
    "# Loss\n",
    "# ----------------------\n",
    "\n",
    "def loss_and_residual_haiku(flat_params, unravel_fn, x_grid):\n",
    "    R = profile_residual_haiku(flat_params, unravel_fn, x_grid)\n",
    "    loss = 0.5 * jnp.mean(R**2)\n",
    "    return loss, R\n",
    "\n",
    "# ----------------------\n",
    "# Initialize params\n",
    "# ----------------------\n",
    "\n",
    "# init in default precision (float32 unless jax_enable_x64 True)\n",
    "init_rng, rng = random.split(rng)\n",
    "dummy_in = jnp.ones((1,1))\n",
    "init_params = net.init(init_rng, dummy_in)\n",
    "flat_init, unravel_fn = ravel_pytree(init_params)\n",
    "\n",
    "# ----------------------\n",
    "# Stage A: Adam pretraining (lower precision / faster)\n",
    "# ----------------------\n",
    "\n",
    "def adam_pretrain(params, x_grid, n_steps=2000, lr=1e-3):\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jit\n",
    "    def step(params, opt_state):\n",
    "        (loss, R), grads = jax.value_and_grad(loss_and_residual_haiku, has_aux=True)(params, unravel_fn, x_grid)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss\n",
    "\n",
    "    p = params\n",
    "    for i in range(n_steps):\n",
    "        p, opt_state, loss = step(p, opt_state)\n",
    "        if i % 200 == 0:\n",
    "            print(f'[Stage A] Adam step {i:5d}  loss={loss:.3e}')\n",
    "    return p\n",
    "\n",
    "# ----------------------\n",
    "# Stage B: Cast to high precision (float64) and run Gauss-Newton\n",
    "# ----------------------\n",
    "\n",
    "# Helper to cast flat params to target dtype\n",
    "\n",
    "def cast_flat_params(flat_params, dtype):\n",
    "    return jax.tree_map(lambda a: a.astype(dtype), flat_params)\n",
    "\n",
    "@partial(jit, static_argnums=(3,))\n",
    "def gn_step_haiku(flat_params, unravel_fn, x_grid, damping=1e-8):\n",
    "    # compute residual and Jacobian J (N x P)\n",
    "    R = profile_residual_haiku(flat_params, unravel_fn, x_grid)\n",
    "    J = jax.jacrev(lambda fp: profile_residual_haiku(fp, unravel_fn, x_grid))(flat_params)\n",
    "\n",
    "    JTJ = J.T @ J\n",
    "    JTr = J.T @ R\n",
    "    dim = JTJ.shape[0]\n",
    "    lhs = JTJ + jnp.eye(dim) * damping\n",
    "    delta = -jnp.linalg.solve(lhs, JTr)\n",
    "    new_flat = flat_params + delta\n",
    "    return new_flat, delta, R\n",
    "\n",
    "# Top-level driver showing the two-stage flow\n",
    "\n",
    "def two_stage_training(x_grid, n_adam=1000, n_gn=6):\n",
    "    # Stage A: Adam pretrain in default precision\n",
    "    print('\n",
    "=== Stage A: Adam pretraining (fast, lower precision) ===')\n",
    "    flat_params, _ = ravel_pytree(init_params)\n",
    "    flat_params = adam_pretrain(flat_params, x_grid, n_steps=n_adam)\n",
    "\n",
    "    # Stage B: enable x64 (do this before casting if necessary). If you want to enable 64-bit\n",
    "    # globally, uncomment the config line at top of file. Otherwise cast params here.\n",
    "    print('\n",
    "=== Stage B: Gauss-Newton refinement (float64) ===')\n",
    "    # Cast to float64 for refinement\n",
    "    flat_params64 = cast_flat_params(flat_params, jnp.float64)\n",
    "\n",
    "    for i in range(n_gn):\n",
    "        flat_params64, delta, R = gn_step_haiku(flat_params64, unravel_fn, x, damping=1e-12)\n",
    "        res_norm = jnp.linalg.norm(R)\n",
    "        delta_norm = jnp.linalg.norm(delta)\n",
    "        print(f'[Stage B] GN iter {i:3d}  res_norm={res_norm:.3e}  delta_norm={delta_norm:.3e}')\n",
    "\n",
    "    return flat_params64\n",
    "\n",
    "# ----------------------\n",
    "# Notes on >64-bit (MPFR / mpmath)\n",
    "# ----------------------\n",
    "# If you require >64-bit precision for the GN linear solve, a practical approach is:\n",
    "# 1. Export the residual function as a Python function that evaluates R at a given parameter\n",
    "#    vector using high-precision arithmetic (mpmath). This requires re-implementing spectral\n",
    "#    FFT/Hilbert in mpmath or evaluating the Fourier series in high precision.\n",
    "# 2. Compute the Jacobian with finite differences in high precision (or use automatic\n",
    "#    differentiation in mpmath-like frameworks) and form the normal equations in high\n",
    "#    precision, then solve for delta using mpmath's linear solver.\n",
    "# This is slower but robust; include it only in the final refinement stages.\n",
    "\n",
    "# ----------------------\n",
    "# If run as script\n",
    "# ----------------------\n",
    "if __name__ == '__main__':\n",
    "    # Use full grid as collocation\n",
    "    x_grid = x\n",
    "    # Stage A + B\n",
    "    final_flat = two_stage_training(x_grid, n_adam=800, n_gn=4)\n",
    "    final_params = unravel_fn(final_flat)\n",
    "    Phi = net_forward(final_params, x)\n",
    "    print('\n",
    "Sample Phi[0:8]=', Phi[:8])\n",
    "    R_final = profile_residual_haiku(final_flat, unravel_fn, x)\n",
    "    print('Final mean |R| =', jnp.mean(jnp.abs(R_final)))\n",
    "\n",
    "# ----------------------\n",
    "# Converting back to Flax or to other refinements:\n",
    "# - Replace dense GN with matrix-free Jvps and CG for large param counts.\n",
    "# - Add spectral dealiasing and filtering to stabilize high-wave-number components.\n",
    "# - Use checkpointing / parameter partitioning if memory is a bottleneck during Jacobian assembly.\n",
    "# ----------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singularity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
